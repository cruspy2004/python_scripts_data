{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5094c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice Data Cleaning Using Pandas\n",
    "\n",
    "This notebook demonstrates data cleaning techniques using the consumer complaints dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c71a168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a0f7b",
   "metadata": {},
   "source": [
    "## 1. Load & Quick Scan\n",
    "\n",
    "Let's start by loading the consumer complaints dataset and performing a quick exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d244e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the consumer complaints dataset...\n",
      "Dataset shape: (2040, 16)\n",
      "Number of rows: 2,040\n",
      "Number of columns: 16\n",
      "\n",
      "Column data types:\n",
      "Complaint ID                      int64\n",
      "Date Received                    object\n",
      "Date Sent to Company             object\n",
      "Product                          object\n",
      "Sub-product                      object\n",
      "Issue                            object\n",
      "Company                          object\n",
      "State                            object\n",
      "ZIP code                         object\n",
      "City                             object\n",
      "Company response to consumer     object\n",
      "Timely response?                 object\n",
      "Consumer disputed?               object\n",
      "Latitude                        float64\n",
      "Longitude                       float64\n",
      "Status                           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Load the dataset into a DataFrame\n",
    "print(\"Loading the consumer complaints dataset...\")\n",
    "df = pd.read_csv(\"consumer_complaints_unclean.csv\")\n",
    "\n",
    "# Task 2: Check shape and column data types\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]:,}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "\n",
    "print(f\"\\nColumn data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d5f9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Complaint ID</th>\n",
       "      <th>Date Received</th>\n",
       "      <th>Date Sent to Company</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>City</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>2023-11-10</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>Interest rate</td>\n",
       "      <td></td>\n",
       "      <td>Metro Loans</td>\n",
       "      <td>NY</td>\n",
       "      <td>75285.0</td>\n",
       "      <td>new york</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.312932</td>\n",
       "      <td>-91.462816</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>Collections</td>\n",
       "      <td>Collection harassment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL</td>\n",
       "      <td>43383.0</td>\n",
       "      <td>miami</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>42.236514</td>\n",
       "      <td>-103.703588</td>\n",
       "      <td>Resolved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>2023-10-20</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td></td>\n",
       "      <td>Interest rate</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NY</td>\n",
       "      <td>43864.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.279018</td>\n",
       "      <td>-94.258898</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>2023-02-17</td>\n",
       "      <td>2023-03-13</td>\n",
       "      <td>Student  loan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fees</td>\n",
       "      <td>Metro Loans</td>\n",
       "      <td>GA</td>\n",
       "      <td>123.0</td>\n",
       "      <td>new york</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>No</td>\n",
       "      <td>34.924120</td>\n",
       "      <td>-85.770266</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>2023-11-23</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Adjustable loan</td>\n",
       "      <td>Collection harassment</td>\n",
       "      <td>United Credit</td>\n",
       "      <td>PA</td>\n",
       "      <td>62086.0</td>\n",
       "      <td>New York</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.643178</td>\n",
       "      <td>-86.962992</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Complaint ID Date Received Date Sent to Company        Product  \\\n",
       "0        100000    2023-11-10           2024-01-01       mortgage   \n",
       "1        100001    2023-03-12           2023-03-21       MORTGAGE   \n",
       "2        100002    2023-10-20           2023-11-27                  \n",
       "3        100003    2023-02-17           2023-03-13  Student  loan   \n",
       "4        100004    2023-11-21           2023-11-23    credit card   \n",
       "\n",
       "       Sub-product                  Issue        Company State  ZIP code  \\\n",
       "0    Interest rate                           Metro Loans    NY   75285.0   \n",
       "1      Collections  Collection harassment            NaN    FL   43383.0   \n",
       "2    Interest rate                  Other            NaN    NY   43864.0   \n",
       "3              NaN                   Fees    Metro Loans    GA     123.0   \n",
       "4  Adjustable loan  Collection harassment  United Credit    PA   62086.0   \n",
       "\n",
       "         City Company response to consumer Timely response?  \\\n",
       "0   new york                           NaN                N   \n",
       "1       miami                          NaN              NaN   \n",
       "2     Unknown                      Unknown            FALSE   \n",
       "3   new york                           NaN          Unknown   \n",
       "4    New York                      Unknown              Yes   \n",
       "\n",
       "  Consumer disputed?   Latitude   Longitude    Status  \n",
       "0                NaN  42.312932  -91.462816    Closed  \n",
       "1                  N  42.236514 -103.703588  Resolved  \n",
       "2                NaN  41.279018  -94.258898    Closed  \n",
       "3                 No  34.924120  -85.770266            \n",
       "4                NaN  35.643178  -86.962992   Unknown  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column names:\n",
      "['Complaint ID', 'Date Received', 'Date Sent to Company', 'Product', 'Sub-product', 'Issue', 'Company', 'State', 'ZIP code', 'City', 'Company response to consumer', 'Timely response?', 'Consumer disputed?', 'Latitude', 'Longitude', 'Status']\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Display the first 5 rows and note any issues\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(\"=\"*50)\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\nColumn names:\")\n",
    "print(list(df.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796758cb",
   "metadata": {},
   "source": [
    "## 2. Missing Values\n",
    "\n",
    "Now let's analyze missing values in the dataset to understand data completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2e4668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Analysis\n",
      "==================================================\n",
      "                                                    Column  Missing_Count  \\\n",
      "Company response to consumer  Company response to consumer            684   \n",
      "Sub-product                                    Sub-product            542   \n",
      "Consumer disputed?                      Consumer disputed?            490   \n",
      "Company                                            Company            359   \n",
      "City                                                  City            292   \n",
      "State                                                State            285   \n",
      "Timely response?                          Timely response?            248   \n",
      "Product                                            Product             79   \n",
      "Issue                                                Issue             70   \n",
      "Date Received                                Date Received              0   \n",
      "Date Sent to Company                  Date Sent to Company              0   \n",
      "Complaint ID                                  Complaint ID              0   \n",
      "ZIP code                                          ZIP code              0   \n",
      "Latitude                                          Latitude              0   \n",
      "Longitude                                        Longitude              0   \n",
      "Status                                              Status              0   \n",
      "\n",
      "                              Missing_Percentage  \n",
      "Company response to consumer               33.53  \n",
      "Sub-product                                26.57  \n",
      "Consumer disputed?                         24.02  \n",
      "Company                                    17.60  \n",
      "City                                       14.31  \n",
      "State                                      13.97  \n",
      "Timely response?                           12.16  \n",
      "Product                                     3.87  \n",
      "Issue                                       3.43  \n",
      "Date Received                               0.00  \n",
      "Date Sent to Company                        0.00  \n",
      "Complaint ID                                0.00  \n",
      "ZIP code                                    0.00  \n",
      "Latitude                                    0.00  \n",
      "Longitude                                   0.00  \n",
      "Status                                      0.00  \n",
      "\n",
      "Example snippet (toy data) - as requested:\n",
      "Toy example missing values:\n",
      "city          1\n",
      "population    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Compute % missing per column\n",
    "print(\"Missing Values Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count missing values per column\n",
    "missing_counts = df.isnull().sum()\n",
    "total_rows = len(df)\n",
    "\n",
    "# Calculate percentage missing\n",
    "missing_percentages = (missing_counts / total_rows) * 100\n",
    "\n",
    "# Create a summary dataframe\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': missing_counts,\n",
    "    'Missing_Percentage': missing_percentages.round(2)\n",
    "})\n",
    "\n",
    "# Sort by missing percentage (descending)\n",
    "missing_summary = missing_summary.sort_values('Missing_Percentage', ascending=False)\n",
    "print(missing_summary)\n",
    "\n",
    "print(f\"\\nExample snippet (toy data) - as requested:\")\n",
    "example = pd.DataFrame({\n",
    "    \"city\": [\"NYC\", None, \"LA\"],\n",
    "    \"population\": [8.4, None, 4.0]\n",
    "})\n",
    "print(\"Toy example missing values:\")\n",
    "print(example.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51670928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with High Missingness (>80%)\n",
      "========================================\n",
      "Found 0 columns with >80% missing values:\n",
      "Empty DataFrame\n",
      "Columns: [Column, Missing_Count, Missing_Percentage]\n",
      "Index: []\n",
      "No columns found with >80% missing values.\n",
      "\n",
      "Columns with moderate missingness (50-80%):\n",
      "Empty DataFrame\n",
      "Columns: [Column, Missing_Count, Missing_Percentage]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Task 5: Identify columns with very high missingness (>80%)\n",
    "print(\"Columns with High Missingness (>80%)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "high_missing_cols = missing_summary[missing_summary['Missing_Percentage'] > 80]\n",
    "print(f\"Found {len(high_missing_cols)} columns with >80% missing values:\")\n",
    "print(high_missing_cols)\n",
    "\n",
    "if len(high_missing_cols) > 0:\n",
    "    print(f\"\\nColumns to consider dropping:\")\n",
    "    for idx, row in high_missing_cols.iterrows():\n",
    "        print(f\"- {row['Column']}: {row['Missing_Percentage']:.1f}% missing\")\n",
    "else:\n",
    "    print(\"No columns found with >80% missing values.\")\n",
    "\n",
    "# Also check for moderately high missingness (>50%)\n",
    "moderate_missing = missing_summary[(missing_summary['Missing_Percentage'] > 50) & \n",
    "                                 (missing_summary['Missing_Percentage'] <= 80)]\n",
    "print(f\"\\nColumns with moderate missingness (50-80%):\")\n",
    "print(moderate_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcb85b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Framework for Missing Data\n",
      "========================================\n",
      "Recommended strategies for each column:\n",
      "                                                    Column  \\\n",
      "Company response to consumer  Company response to consumer   \n",
      "Sub-product                                    Sub-product   \n",
      "Consumer disputed?                      Consumer disputed?   \n",
      "Company                                            Company   \n",
      "City                                                  City   \n",
      "State                                                State   \n",
      "Timely response?                          Timely response?   \n",
      "Product                                            Product   \n",
      "Issue                                                Issue   \n",
      "Date Received                                Date Received   \n",
      "Date Sent to Company                  Date Sent to Company   \n",
      "Complaint ID                                  Complaint ID   \n",
      "ZIP code                                          ZIP code   \n",
      "Latitude                                          Latitude   \n",
      "Longitude                                        Longitude   \n",
      "Status                                              Status   \n",
      "\n",
      "                              Missing_Percentage  \\\n",
      "Company response to consumer               33.53   \n",
      "Sub-product                                26.57   \n",
      "Consumer disputed?                         24.02   \n",
      "Company                                    17.60   \n",
      "City                                       14.31   \n",
      "State                                      13.97   \n",
      "Timely response?                           12.16   \n",
      "Product                                     3.87   \n",
      "Issue                                       3.43   \n",
      "Date Received                               0.00   \n",
      "Date Sent to Company                        0.00   \n",
      "Complaint ID                                0.00   \n",
      "ZIP code                                    0.00   \n",
      "Latitude                                    0.00   \n",
      "Longitude                                   0.00   \n",
      "Status                                      0.00   \n",
      "\n",
      "                                             Recommended_Strategy  \n",
      "Company response to consumer     FILL - Use imputation strategies  \n",
      "Sub-product                      FILL - Use imputation strategies  \n",
      "Consumer disputed?               FILL - Use imputation strategies  \n",
      "Company                       FILL - Simple filling or keep as-is  \n",
      "City                          FILL - Simple filling or keep as-is  \n",
      "State                         FILL - Simple filling or keep as-is  \n",
      "Timely response?              FILL - Simple filling or keep as-is  \n",
      "Product                       FILL - Simple filling or keep as-is  \n",
      "Issue                         FILL - Simple filling or keep as-is  \n",
      "Date Received                            KEEP - No missing values  \n",
      "Date Sent to Company                     KEEP - No missing values  \n",
      "Complaint ID                             KEEP - No missing values  \n",
      "ZIP code                                 KEEP - No missing values  \n",
      "Latitude                                 KEEP - No missing values  \n",
      "Longitude                                KEEP - No missing values  \n",
      "Status                                   KEEP - No missing values  \n",
      "\n",
      "Summary of recommended actions:\n",
      "- KEEP - No missing values: 7 columns\n",
      "- FILL - Simple filling or keep as-is: 6 columns\n",
      "- FILL - Use imputation strategies: 3 columns\n"
     ]
    }
   ],
   "source": [
    "# Task 6: Decide whether to drop, fill, or keep columns\n",
    "print(\"Decision Framework for Missing Data\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "def missing_data_strategy(missing_pct, column_name):\n",
    "    \"\"\"Determine strategy based on missing percentage\"\"\"\n",
    "    if missing_pct > 80:\n",
    "        return \"DROP - Too much missing data\"\n",
    "    elif missing_pct > 50:\n",
    "        return \"EVALUATE - Consider importance vs missingness\"\n",
    "    elif missing_pct > 20:\n",
    "        return \"FILL - Use imputation strategies\"\n",
    "    elif missing_pct > 0:\n",
    "        return \"FILL - Simple filling or keep as-is\"\n",
    "    else:\n",
    "        return \"KEEP - No missing values\"\n",
    "\n",
    "# Apply strategy to each column\n",
    "strategies = []\n",
    "for idx, row in missing_summary.iterrows():\n",
    "    strategy = missing_data_strategy(row['Missing_Percentage'], row['Column'])\n",
    "    strategies.append(strategy)\n",
    "\n",
    "missing_summary['Recommended_Strategy'] = strategies\n",
    "\n",
    "# Display recommendations\n",
    "print(\"Recommended strategies for each column:\")\n",
    "print(missing_summary[['Column', 'Missing_Percentage', 'Recommended_Strategy']])\n",
    "\n",
    "# Summary of actions\n",
    "print(f\"\\nSummary of recommended actions:\")\n",
    "strategy_counts = missing_summary['Recommended_Strategy'].value_counts()\n",
    "for strategy, count in strategy_counts.items():\n",
    "    print(f\"- {strategy}: {count} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72869b0",
   "metadata": {},
   "source": [
    "## 3. Duplicates\n",
    "\n",
    "Let's identify and handle duplicate records in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb86093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Analysis\n",
      "==============================\n",
      "Total duplicate rows: 40\n",
      "Percentage of duplicates: 1.96%\n",
      "Duplicate Complaint IDs: 155\n",
      "\n",
      "Example duplicate rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Complaint ID</th>\n",
       "      <th>Date Received</th>\n",
       "      <th>Date Sent to Company</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>City</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>100056</td>\n",
       "      <td>2023-09-18</td>\n",
       "      <td>2023-11-04</td>\n",
       "      <td>CREDIT reporting</td>\n",
       "      <td>Collections</td>\n",
       "      <td>Collection harassment</td>\n",
       "      <td>Metro Loans</td>\n",
       "      <td>IL</td>\n",
       "      <td>28577</td>\n",
       "      <td>miami</td>\n",
       "      <td>Closed</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>39.532682</td>\n",
       "      <td>-117.441373</td>\n",
       "      <td>In Progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>100231</td>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>2024-02-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rewards</td>\n",
       "      <td>Fees</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CA</td>\n",
       "      <td>10694</td>\n",
       "      <td>Detroit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>N</td>\n",
       "      <td>34.016012</td>\n",
       "      <td>-87.405957</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>100275</td>\n",
       "      <td>2024-04-24</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>credit reporting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fees</td>\n",
       "      <td>United Credit</td>\n",
       "      <td></td>\n",
       "      <td>64311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Yes</td>\n",
       "      <td>43.679343</td>\n",
       "      <td>-97.452391</td>\n",
       "      <td>Resolved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>100367</td>\n",
       "      <td>2023-06-11</td>\n",
       "      <td>2023-07-10</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Identity theft</td>\n",
       "      <td>Metro Loans</td>\n",
       "      <td>NC</td>\n",
       "      <td>61792</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.674465</td>\n",
       "      <td>-86.431002</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>100375</td>\n",
       "      <td>2023-12-10</td>\n",
       "      <td>2023-12-11</td>\n",
       "      <td>bank Account</td>\n",
       "      <td>Interest rate</td>\n",
       "      <td>Identity theft</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OH</td>\n",
       "      <td>47709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In progress</td>\n",
       "      <td>Y</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>45.813869</td>\n",
       "      <td>-100.712525</td>\n",
       "      <td>In Progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>100442</td>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>2023-03-11</td>\n",
       "      <td>Credit  Card</td>\n",
       "      <td>Adjustable loan</td>\n",
       "      <td>Billing disputes</td>\n",
       "      <td>United Credit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24733</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>No</td>\n",
       "      <td>Y</td>\n",
       "      <td>44.743886</td>\n",
       "      <td>-90.943190</td>\n",
       "      <td>Open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>100447</td>\n",
       "      <td>2023-07-23</td>\n",
       "      <td>2023-08-02</td>\n",
       "      <td>CREDIT reporting</td>\n",
       "      <td>Interest rate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IL</td>\n",
       "      <td>93070</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.796463</td>\n",
       "      <td>-104.006724</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>100558</td>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>2023-09-18</td>\n",
       "      <td>CREDIT CARD</td>\n",
       "      <td>Collections</td>\n",
       "      <td>Billing disputes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GA</td>\n",
       "      <td>98437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>45.884184</td>\n",
       "      <td>-100.004280</td>\n",
       "      <td>Open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>100573</td>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>2023-12-16</td>\n",
       "      <td>bank Account</td>\n",
       "      <td>Adjustable loan</td>\n",
       "      <td>Fees</td>\n",
       "      <td>Metro Loans</td>\n",
       "      <td>NC</td>\n",
       "      <td>36811</td>\n",
       "      <td>New York</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Yes</td>\n",
       "      <td>50.017163</td>\n",
       "      <td>-84.602810</td>\n",
       "      <td>Resolved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>100577</td>\n",
       "      <td>2023-01-22</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>Credit Reporting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Incorrect information</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>OH</td>\n",
       "      <td>87785</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>43.118418</td>\n",
       "      <td>-88.159617</td>\n",
       "      <td>In Progress</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Complaint ID Date Received Date Sent to Company           Product  \\\n",
       "56         100056    2023-09-18           2023-11-04  CREDIT reporting   \n",
       "231        100231    2024-01-05           2024-02-27               NaN   \n",
       "275        100275    2024-04-24           2024-04-25  credit reporting   \n",
       "367        100367    2023-06-11           2023-07-10          mortgage   \n",
       "375        100375    2023-12-10           2023-12-11      bank Account   \n",
       "442        100442    2023-03-10           2023-03-11      Credit  Card   \n",
       "447        100447    2023-07-23           2023-08-02  CREDIT reporting   \n",
       "558        100558    2023-08-18           2023-09-18       CREDIT CARD   \n",
       "573        100573    2023-12-09           2023-12-16      bank Account   \n",
       "577        100577    2023-01-22           2023-03-01  Credit Reporting   \n",
       "\n",
       "         Sub-product                  Issue        Company State ZIP code  \\\n",
       "56       Collections  Collection harassment    Metro Loans    IL    28577   \n",
       "231          Rewards                   Fees            NaN    CA    10694   \n",
       "275              NaN                   Fees  United Credit          64311   \n",
       "367              NaN         Identity theft    Metro Loans    NC    61792   \n",
       "375    Interest rate         Identity theft            NaN    OH    47709   \n",
       "442  Adjustable loan       Billing disputes  United Credit   NaN    24733   \n",
       "447    Interest rate                    NaN            NaN    IL    93070   \n",
       "558      Collections       Billing disputes            NaN    GA    98437   \n",
       "573  Adjustable loan                   Fees    Metro Loans    NC    36811   \n",
       "577              NaN  Incorrect information        Unknown    OH    87785   \n",
       "\n",
       "          City Company response to consumer Timely response?  \\\n",
       "56       miami                       Closed               No   \n",
       "231    Detroit                          NaN              Yes   \n",
       "275        NaN                          NaN                N   \n",
       "367    Unknown                      Unknown            FALSE   \n",
       "375        NaN                  In progress                Y   \n",
       "442  Charlotte      Closed with explanation               No   \n",
       "447    Chicago                      Unknown              NaN   \n",
       "558        NaN                          NaN              NaN   \n",
       "573   New York                          NaN          Unknown   \n",
       "577    Unknown                          NaN              Yes   \n",
       "\n",
       "    Consumer disputed?   Latitude   Longitude       Status  \n",
       "56                  No  39.532682 -117.441373  In Progress  \n",
       "231                  N  34.016012  -87.405957      Unknown  \n",
       "275                Yes  43.679343  -97.452391     Resolved  \n",
       "367                NaN  35.674465  -86.431002               \n",
       "375              FALSE  45.813869 -100.712525  In Progress  \n",
       "442                  Y  44.743886  -90.943190         Open  \n",
       "447                NaN  35.796463 -104.006724               \n",
       "558               TRUE  45.884184 -100.004280         Open  \n",
       "573                Yes  50.017163  -84.602810     Resolved  \n",
       "577                 No  43.118418  -88.159617  In Progress  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example snippet (toy data) - as requested:\n",
      "Toy example duplicates:\n",
      "Number of duplicates: 1\n",
      "After dropping duplicates:\n",
      "   A  B\n",
      "0  1  x\n",
      "2  2  y\n"
     ]
    }
   ],
   "source": [
    "# Task 7: Count the number of duplicate rows\n",
    "print(\"Duplicate Analysis\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Count total duplicate rows\n",
    "total_duplicates = df.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {total_duplicates}\")\n",
    "print(f\"Percentage of duplicates: {(total_duplicates/len(df)*100):.2f}%\")\n",
    "\n",
    "# Check duplicates based on Complaint ID (should be unique)\n",
    "complaint_id_duplicates = df.duplicated(subset=['Complaint ID']).sum()\n",
    "print(f\"Duplicate Complaint IDs: {complaint_id_duplicates}\")\n",
    "\n",
    "# Show some examples of duplicates if they exist\n",
    "if total_duplicates > 0:\n",
    "    print(f\"\\nExample duplicate rows:\")\n",
    "    duplicate_rows = df[df.duplicated(keep=False)].head(10)\n",
    "    display(duplicate_rows)\n",
    "\n",
    "# Example snippet (toy data) - as requested\n",
    "print(f\"\\nExample snippet (toy data) - as requested:\")\n",
    "toy_df = pd.DataFrame({\"A\": [1, 1, 2], \"B\": [\"x\", \"x\", \"y\"]})\n",
    "print(\"Toy example duplicates:\")\n",
    "print(f\"Number of duplicates: {toy_df.duplicated().sum()}\")\n",
    "print(\"After dropping duplicates:\")\n",
    "print(toy_df.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77822a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Duplicates\n",
      "=========================\n",
      "Shape before removing duplicates: (2040, 16)\n",
      "Shape after removing duplicates: (1885, 16)\n",
      "Rows removed: 155\n",
      "Updated dataframe shape: (1885, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haadh\\AppData\\Local\\Temp\\ipykernel_14164\\2307954289.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Date Received'] = df_cleaned['Date Received'].dt.strftime('%Y-%m-%d')\n"
     ]
    }
   ],
   "source": [
    "# Task 8: Drop duplicates by Complaint ID, keeping the latest Date Received\n",
    "print(\"Removing Duplicates\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# Report shape before\n",
    "print(f\"Shape before removing duplicates: {df.shape}\")\n",
    "\n",
    "# First, let's convert Date Received to datetime for proper sorting (temporarily)\n",
    "df_temp = df.copy()\n",
    "df_temp['Date Received'] = pd.to_datetime(df_temp['Date Received'], errors='coerce')\n",
    "\n",
    "# Sort by Complaint ID and Date Received (descending to get latest dates first)\n",
    "df_sorted = df_temp.sort_values(['Complaint ID', 'Date Received'], ascending=[True, False])\n",
    "\n",
    "# Drop duplicates based on Complaint ID, keeping the first occurrence (which is the latest date)\n",
    "df_cleaned = df_sorted.drop_duplicates(subset=['Complaint ID'], keep='first')\n",
    "\n",
    "# Convert back to original format to maintain consistency\n",
    "df_cleaned['Date Received'] = df_cleaned['Date Received'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Task 9: Report shape before and after\n",
    "print(f\"Shape after removing duplicates: {df_cleaned.shape}\")\n",
    "print(f\"Rows removed: {df.shape[0] - df_cleaned.shape[0]}\")\n",
    "\n",
    "# Update our main dataframe\n",
    "df = df_cleaned.reset_index(drop=True)\n",
    "print(f\"Updated dataframe shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccc4f4",
   "metadata": {},
   "source": [
    "## 4. Data Types & Parsing\n",
    "\n",
    "Now let's convert columns to their appropriate data types for better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc9a29a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type Conversions\n",
      "==============================\n",
      "Original data types:\n",
      "Complaint ID                      int64\n",
      "Date Received                    object\n",
      "Date Sent to Company             object\n",
      "Product                          object\n",
      "Sub-product                      object\n",
      "Issue                            object\n",
      "Company                          object\n",
      "State                            object\n",
      "ZIP code                         object\n",
      "City                             object\n",
      "Company response to consumer     object\n",
      "Timely response?                 object\n",
      "Consumer disputed?               object\n",
      "Latitude                        float64\n",
      "Longitude                       float64\n",
      "Status                           object\n",
      "dtype: object\n",
      "\n",
      "Converting date columns to datetime...\n",
      "Date Received parsing errors: 0\n",
      "Date Sent to Company parsing errors: 0\n",
      "\n",
      "Example snippet (toy data) - as requested:\n",
      "Before conversion:\n",
      "date    object\n",
      "zip     object\n",
      "dtype: object\n",
      "After conversion:\n",
      "date    datetime64[ns]\n",
      "zip            float64\n",
      "dtype: object\n",
      "        date      zip\n",
      "0 2020-01-01  10001.0\n",
      "1        NaT      NaN\n"
     ]
    }
   ],
   "source": [
    "# Task 10: Convert Date Received and Date Sent to Company to datetime\n",
    "print(\"Data Type Conversions\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"Original data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convert date columns to datetime\n",
    "print(f\"\\nConverting date columns to datetime...\")\n",
    "df['Date Received'] = pd.to_datetime(df['Date Received'], errors='coerce')\n",
    "df['Date Sent to Company'] = pd.to_datetime(df['Date Sent to Company'], errors='coerce')\n",
    "\n",
    "# Check for any parsing errors\n",
    "date_received_errors = df['Date Received'].isnull().sum()\n",
    "date_sent_errors = df['Date Sent to Company'].isnull().sum()\n",
    "\n",
    "print(f\"Date Received parsing errors: {date_received_errors}\")\n",
    "print(f\"Date Sent to Company parsing errors: {date_sent_errors}\")\n",
    "\n",
    "# Example snippet (toy data) - as requested\n",
    "print(f\"\\nExample snippet (toy data) - as requested:\")\n",
    "example = pd.DataFrame({\n",
    "    \"date\": [\"2020-01-01\", \"bad\"],\n",
    "    \"zip\": [\"10001\", \"abc\"]\n",
    "})\n",
    "print(\"Before conversion:\")\n",
    "print(example.dtypes)\n",
    "example['date'] = pd.to_datetime(example['date'], errors='coerce')\n",
    "example['zip'] = pd.to_numeric(example['zip'], errors='coerce')\n",
    "print(\"After conversion:\")\n",
    "print(example.dtypes)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d11b854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting numeric columns...\n",
      "ZIP code before conversion: object\n",
      "ZIP code conversion errors: 159\n",
      "Latitude before conversion: float64\n",
      "Longitude before conversion: float64\n",
      "Latitude conversion errors: 0\n",
      "Longitude conversion errors: 0\n",
      "After numeric conversions:\n",
      "ZIP code: float64\n",
      "Latitude: float64\n",
      "Longitude: float64\n"
     ]
    }
   ],
   "source": [
    "# Task 11: Convert ZIP code, Latitude, Longitude to numeric\n",
    "print(\"Converting numeric columns...\")\n",
    "\n",
    "# Convert ZIP code to numeric (handle non-numeric values)\n",
    "print(f\"ZIP code before conversion: {df['ZIP code'].dtype}\")\n",
    "df['ZIP code'] = pd.to_numeric(df['ZIP code'], errors='coerce')\n",
    "zip_errors = df['ZIP code'].isnull().sum()\n",
    "print(f\"ZIP code conversion errors: {zip_errors}\")\n",
    "\n",
    "# Convert Latitude and Longitude to numeric (they should already be numeric)\n",
    "print(f\"Latitude before conversion: {df['Latitude'].dtype}\")\n",
    "print(f\"Longitude before conversion: {df['Longitude'].dtype}\")\n",
    "\n",
    "df['Latitude'] = pd.to_numeric(df['Latitude'], errors='coerce')\n",
    "df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')\n",
    "\n",
    "lat_errors = df['Latitude'].isnull().sum() \n",
    "lon_errors = df['Longitude'].isnull().sum()\n",
    "print(f\"Latitude conversion errors: {lat_errors}\")\n",
    "print(f\"Longitude conversion errors: {lon_errors}\")\n",
    "\n",
    "print(f\"After numeric conversions:\")\n",
    "print(f\"ZIP code: {df['ZIP code'].dtype}\")\n",
    "print(f\"Latitude: {df['Latitude'].dtype}\")\n",
    "print(f\"Longitude: {df['Longitude'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c89dae00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting categorical columns...\n",
      "Converting 10 columns to categorical:\n",
      "- Product: object -> category\n",
      "- State: object -> category\n",
      "- City: object -> category\n",
      "- Company: object -> category\n",
      "- Sub-product: object -> category\n",
      "- Issue: object -> category\n",
      "- Company response to consumer: object -> category\n",
      "- Timely response?: object -> category\n",
      "- Consumer disputed?: object -> category\n",
      "- Status: object -> category\n",
      "\n",
      "Final data types after all conversions:\n",
      "========================================\n",
      "Complaint ID                             int64\n",
      "Date Received                   datetime64[ns]\n",
      "Date Sent to Company            datetime64[ns]\n",
      "Product                               category\n",
      "Sub-product                           category\n",
      "Issue                                 category\n",
      "Company                               category\n",
      "State                                 category\n",
      "ZIP code                               float64\n",
      "City                                  category\n",
      "Company response to consumer          category\n",
      "Timely response?                      category\n",
      "Consumer disputed?                    category\n",
      "Latitude                               float64\n",
      "Longitude                              float64\n",
      "Status                                category\n",
      "dtype: object\n",
      "\n",
      "Dataset info after type conversions:\n",
      "Shape: (1885, 16)\n",
      "Memory usage: 0.11 MB\n"
     ]
    }
   ],
   "source": [
    "# Task 12: Change Product, State, City to categorical/string\n",
    "print(\"Converting categorical columns...\")\n",
    "\n",
    "# Convert to categorical for better memory usage and performance\n",
    "categorical_columns = ['Product', 'State', 'City', 'Company', 'Sub-product', \n",
    "                      'Issue', 'Company response to consumer', 'Timely response?', \n",
    "                      'Consumer disputed?', 'Status']\n",
    "\n",
    "print(f\"Converting {len(categorical_columns)} columns to categorical:\")\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"- {col}: {df[col].dtype} -> category\")\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "# Final data types summary\n",
    "print(f\"\\nFinal data types after all conversions:\")\n",
    "print(\"=\"*40)\n",
    "print(df.dtypes)\n",
    "\n",
    "# Memory usage comparison would be good to show\n",
    "print(f\"\\nDataset info after type conversions:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e46612",
   "metadata": {},
   "source": [
    "## 5. Renaming & Category Standardization\n",
    "\n",
    "Let's standardize column names and clean up categorical data for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ea1067e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming Columns to snake_case\n",
      "===================================\n",
      "Column name conversions:\n",
      "- 'Complaint ID' -> 'complaint_id'\n",
      "- 'Date Received' -> 'date_received'\n",
      "- 'Date Sent to Company' -> 'date_sent_to_company'\n",
      "- 'Product' -> 'product'\n",
      "- 'Sub-product' -> 'sub_product'\n",
      "- 'Issue' -> 'issue'\n",
      "- 'Company' -> 'company'\n",
      "- 'State' -> 'state'\n",
      "- 'ZIP code' -> 'zip_code'\n",
      "- 'City' -> 'city'\n",
      "- 'Company response to consumer' -> 'company_response_to_consumer'\n",
      "- 'Timely response?' -> 'timely_response?'\n",
      "- 'Consumer disputed?' -> 'consumer_disputed?'\n",
      "- 'Latitude' -> 'latitude'\n",
      "- 'Longitude' -> 'longitude'\n",
      "- 'Status' -> 'status'\n",
      "\n",
      "New column names:\n",
      "['complaint_id', 'date_received', 'date_sent_to_company', 'product', 'sub_product', 'issue', 'company', 'state', 'zip_code', 'city', 'company_response_to_consumer', 'timely_response?', 'consumer_disputed?', 'latitude', 'longitude', 'status']\n",
      "\n",
      "Example snippet (toy data) - as requested:\n",
      "Toy example column renaming:\n",
      "['created_date']\n"
     ]
    }
   ],
   "source": [
    "# Task 13: Rename all columns to lowercase snake_case\n",
    "print(\"Renaming Columns to snake_case\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Function to convert column names to snake_case\n",
    "def to_snake_case(name):\n",
    "    \"\"\"Convert column name to lowercase snake_case\"\"\"\n",
    "    import re\n",
    "    # Replace spaces and hyphens with underscores\n",
    "    name = re.sub(r'[\\s\\-]+', '_', name)\n",
    "    # Convert to lowercase\n",
    "    name = name.lower()\n",
    "    # Remove any duplicate underscores\n",
    "    name = re.sub(r'_+', '_', name)\n",
    "    # Remove leading/trailing underscores\n",
    "    name = name.strip('_')\n",
    "    return name\n",
    "\n",
    "# Show before and after column names\n",
    "print(\"Column name conversions:\")\n",
    "old_columns = df.columns.tolist()\n",
    "new_columns = [to_snake_case(col) for col in old_columns]\n",
    "\n",
    "for old, new in zip(old_columns, new_columns):\n",
    "    if old != new:\n",
    "        print(f\"- '{old}' -> '{new}'\")\n",
    "\n",
    "# Apply the renaming\n",
    "column_mapping = dict(zip(old_columns, new_columns))\n",
    "df = df.rename(columns=column_mapping)\n",
    "\n",
    "print(f\"\\nNew column names:\")\n",
    "print(list(df.columns))\n",
    "\n",
    "# Example snippet (toy data) - as requested\n",
    "print(f\"\\nExample snippet (toy data) - as requested:\")\n",
    "example = pd.DataFrame({\"Created Date\": [1, 2]})\n",
    "example = example.rename(columns={\"Created Date\": \"created_date\"})\n",
    "print(\"Toy example column renaming:\")\n",
    "print(example.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6609da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing Categorical Data\n",
      "===================================\n",
      "Cleaning categorical columns:\n",
      "- Cleaning product...\n",
      "  Before: ['mortgage' 'MORTGAGE' ' ' 'Student  loan' 'credit card']\n",
      "  After:  ['Mortgage' '' 'Student  Loan' 'Credit Card' 'Credit  Card']\n",
      "- Cleaning state...\n",
      "  Before: ['NY' 'FL' 'GA' 'PA' 'CA']\n",
      "  After:  ['Ny' 'Fl' 'Ga' 'Pa' 'Ca']\n",
      "- Cleaning city...\n",
      "  Before: [' new york ' 'miami' 'Unknown' 'New York' 'Miami']\n",
      "  After:  ['New York' 'Miami' 'Unknown' 'Nan' 'Detroit']\n",
      "- Cleaning company...\n",
      "  Before: ['Metro Loans' np.str_('nan') 'United Credit' 'Acme Bank'\n",
      " 'Northstar Finance']\n",
      "  After:  ['Metro Loans' 'Nan' 'United Credit' 'Acme Bank' 'Northstar Finance']\n",
      "- Cleaning sub_product...\n",
      "  Before: ['Interest rate' 'Collections' np.str_('nan') 'Adjustable loan'\n",
      " 'Fixed loan']\n",
      "  After:  ['Interest Rate' 'Collections' 'Nan' 'Adjustable Loan' 'Fixed Loan']\n",
      "- Cleaning issue...\n",
      "  Before: [' ' 'Collection harassment' 'Other' 'Fees' 'Incorrect information']\n",
      "  After:  ['' 'Collection Harassment' 'Other' 'Fees' 'Incorrect Information']\n",
      "\n",
      "Example snippet (toy data) - as requested:\n",
      "Toy example category cleaning:\n",
      "Before: [' nyc ', 'NYC', 'new york']\n",
      "After:  ['Nyc', 'Nyc', 'New York']\n"
     ]
    }
   ],
   "source": [
    "# Task 14: Standardize Product, State, City categories (trim spaces, title case)\n",
    "print(\"Standardizing Categorical Data\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# List of categorical columns to standardize\n",
    "categorical_cols_to_clean = ['product', 'state', 'city', 'company', 'sub_product', 'issue']\n",
    "\n",
    "# Function to clean categorical data\n",
    "def clean_categorical(series):\n",
    "    \"\"\"Clean categorical data by trimming spaces and applying title case\"\"\"\n",
    "    if series.dtype.name == 'category':\n",
    "        # Convert to string temporarily for cleaning\n",
    "        cleaned = series.astype(str).str.strip().str.title()\n",
    "        return cleaned.astype('category')\n",
    "    else:\n",
    "        return series.str.strip().str.title()\n",
    "\n",
    "print(\"Cleaning categorical columns:\")\n",
    "for col in categorical_cols_to_clean:\n",
    "    if col in df.columns:\n",
    "        print(f\"- Cleaning {col}...\")\n",
    "        # Show some examples before cleaning\n",
    "        unique_before = df[col].astype(str).unique()[:5]\n",
    "        \n",
    "        # Clean the column\n",
    "        df[col] = clean_categorical(df[col])\n",
    "        \n",
    "        # Show examples after cleaning\n",
    "        unique_after = df[col].astype(str).unique()[:5]\n",
    "        \n",
    "        print(f\"  Before: {unique_before}\")\n",
    "        print(f\"  After:  {unique_after}\")\n",
    "\n",
    "# Example snippet (toy data) - as requested\n",
    "print(f\"\\nExample snippet (toy data) - as requested:\")\n",
    "cats = pd.Series([\" nyc \", \"NYC\", \"new york\"])\n",
    "cleaned_cats = cats.str.strip().str.title()\n",
    "print(\"Toy example category cleaning:\")\n",
    "print(\"Before:\", cats.tolist())\n",
    "print(\"After: \", cleaned_cats.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e97ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Inconsistent Product Spellings\n",
      "========================================\n",
      "Current Product categories:\n",
      "product\n",
      "Mortgage            294\n",
      "Bank Account        282\n",
      "Credit Reporting    265\n",
      "Credit Card         261\n",
      "Student Loan        188\n",
      "Debt Collection     185\n",
      "Credit  Card        101\n",
      "Student  Loan        86\n",
      "Debt  Collection     79\n",
      "Nan                  70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying product standardization...\n",
      "- Standardizing 'Credit Card' -> 'Credit Card' (261 records)\n",
      "- Standardizing 'Mortgage' -> 'Mortgage' (294 records)\n",
      "- Standardizing 'Student Loan' -> 'Student Loan' (188 records)\n",
      "- Standardizing 'Debt Collection' -> 'Debt Collection' (185 records)\n",
      "\n",
      "Product categories after standardization:\n",
      "product\n",
      "Mortgage            294\n",
      "Bank Account        282\n",
      "Credit Reporting    265\n",
      "Credit Card         261\n",
      "Student Loan        188\n",
      "Debt Collection     185\n",
      "Credit  Card        101\n",
      "Student  Loan        86\n",
      "Debt  Collection     79\n",
      "Nan                  70\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Task 15: Fix inconsistent spellings in Product (e.g., Credit card vs CREDIT CARD)\n",
    "print(\"Fixing Inconsistent Product Spellings\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Check current product categories\n",
    "print(\"Current Product categories:\")\n",
    "product_counts = df['product'].value_counts()\n",
    "print(product_counts.head(10))\n",
    "\n",
    "# Create a mapping for common inconsistencies\n",
    "product_mappings = {\n",
    "    'Credit Card': 'Credit Card',\n",
    "    'Credit Cards': 'Credit Card',\n",
    "    'Creditcard': 'Credit Card',\n",
    "    'Credit Card Or Prepaid Card': 'Credit Card Or Prepaid Card',\n",
    "    'Mortgage': 'Mortgage',\n",
    "    'Mortgages': 'Mortgage',\n",
    "    'Student Loan': 'Student Loan',\n",
    "    'Student Loans': 'Student Loan',\n",
    "    'Bank Account Or Service': 'Bank Account Or Service',\n",
    "    'Banking': 'Bank Account Or Service',\n",
    "    'Debt Collection': 'Debt Collection',\n",
    "    'Debt Collections': 'Debt Collection'\n",
    "}\n",
    "\n",
    "# Apply standardization\n",
    "print(f\"\\nApplying product standardization...\")\n",
    "df['product'] = df['product'].astype(str)\n",
    "\n",
    "# Apply mappings\n",
    "for old_name, new_name in product_mappings.items():\n",
    "    mask = df['product'].str.contains(old_name, case=False, na=False)\n",
    "    if mask.any():\n",
    "        count = mask.sum()\n",
    "        print(f\"- Standardizing '{old_name}' -> '{new_name}' ({count} records)\")\n",
    "        df.loc[mask, 'product'] = new_name\n",
    "\n",
    "# Convert back to category\n",
    "df['product'] = df['product'].astype('category')\n",
    "\n",
    "print(f\"\\nProduct categories after standardization:\")\n",
    "print(df['product'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be4ba3",
   "metadata": {},
   "source": [
    "## 6. Null-like Tokens, Outliers & Sanity Checks\n",
    "\n",
    "Let's identify and handle null-like values, outliers, and perform data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc077c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing Null-like Tokens\n",
      "==============================\n",
      "Null-like tokens to replace: ['N/A', 'Unknown', '', 'null', 'NULL', 'None', 'NONE', 'n/a', 'unknown', 'Not Available', 'Not Provided', 'Not Specified', ' ', '  ']\n",
      "\n",
      "Null-like tokens found per column:\n",
      "- product: 74 null-like tokens\n",
      "- issue: 59 null-like tokens\n",
      "- company: 327 null-like tokens\n",
      "- state: 263 null-like tokens\n",
      "- city: 188 null-like tokens\n",
      "- company_response_to_consumer: 290 null-like tokens\n",
      "- timely_response?: 238 null-like tokens\n",
      "- status: 648 null-like tokens\n",
      "\n",
      "Replaced null-like tokens in 8 columns\n",
      "Total replacements made: 2087\n",
      "\n",
      "Example snippet (toy data) - as requested:\n",
      "Toy example null token replacement:\n",
      "Before: ['N/A', 'Unknown', 'NYC']\n",
      "After:  [<NA>, <NA>, 'NYC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haadh\\AppData\\Local\\Temp\\ipykernel_14164\\539959309.py:30: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df[col] = df[col].replace(null_tokens, pd.NA)\n",
      "C:\\Users\\haadh\\AppData\\Local\\Temp\\ipykernel_14164\\539959309.py:30: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df[col] = df[col].replace(null_tokens, pd.NA)\n",
      "C:\\Users\\haadh\\AppData\\Local\\Temp\\ipykernel_14164\\539959309.py:30: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df[col] = df[col].replace(null_tokens, pd.NA)\n",
      "C:\\Users\\haadh\\AppData\\Local\\Temp\\ipykernel_14164\\539959309.py:30: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df[col] = df[col].replace(null_tokens, pd.NA)\n",
      "C:\\Users\\haadh\\AppData\\Local\\Temp\\ipykernel_14164\\539959309.py:30: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df[col] = df[col].replace(null_tokens, pd.NA)\n",
      "C:\\Users\\haadh\\AppData\\Local\\Temp\\ipykernel_14164\\539959309.py:30: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df[col] = df[col].replace(null_tokens, pd.NA)\n",
      "C:\\Users\\haadh\\AppData\\Local\\Temp\\ipykernel_14164\\539959309.py:30: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df[col] = df[col].replace(null_tokens, pd.NA)\n",
      "C:\\Users\\haadh\\AppData\\Local\\Temp\\ipykernel_14164\\539959309.py:30: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df[col] = df[col].replace(null_tokens, pd.NA)\n"
     ]
    }
   ],
   "source": [
    "# Task 16: Replace null-like tokens ('N/A','Unknown','') with NaN\n",
    "print(\"Replacing Null-like Tokens\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Define null-like tokens to replace\n",
    "null_tokens = ['N/A', 'Unknown', '', 'null', 'NULL', 'None', 'NONE', 'n/a', 'unknown', \n",
    "               'Not Available', 'Not Provided', 'Not Specified', ' ', '  ']\n",
    "\n",
    "print(f\"Null-like tokens to replace: {null_tokens}\")\n",
    "\n",
    "# Function to count null-like tokens in a column\n",
    "def count_null_tokens(series, tokens):\n",
    "    \"\"\"Count null-like tokens in a series\"\"\"\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if series.dtype == 'object' or series.dtype.name == 'category':\n",
    "            count += (series.astype(str) == token).sum()\n",
    "    return count\n",
    "\n",
    "# Check each column for null-like tokens\n",
    "print(f\"\\nNull-like tokens found per column:\")\n",
    "columns_to_check = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "replacements_made = {}\n",
    "for col in columns_to_check:\n",
    "    token_count = count_null_tokens(df[col], null_tokens)\n",
    "    if token_count > 0:\n",
    "        print(f\"- {col}: {token_count} null-like tokens\")\n",
    "        # Replace tokens with NaN\n",
    "        df[col] = df[col].replace(null_tokens, pd.NA)\n",
    "        replacements_made[col] = token_count\n",
    "\n",
    "if replacements_made:\n",
    "    print(f\"\\nReplaced null-like tokens in {len(replacements_made)} columns\")\n",
    "    total_replacements = sum(replacements_made.values())\n",
    "    print(f\"Total replacements made: {total_replacements}\")\n",
    "else:\n",
    "    print(\"No null-like tokens found to replace\")\n",
    "\n",
    "# Example snippet (toy data) - as requested\n",
    "print(f\"\\nExample snippet (toy data) - as requested:\")\n",
    "s = pd.Series([\"N/A\", \"Unknown\", \"NYC\"])\n",
    "cleaned_s = s.replace([\"N/A\", \"Unknown\"], pd.NA)\n",
    "print(\"Toy example null token replacement:\")\n",
    "print(\"Before:\", s.tolist())\n",
    "print(\"After: \", cleaned_s.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f21e37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier Detection in Geographic Coordinates\n",
      "=============================================\n",
      "- latitude:\n",
      "  Q1: 34.6489, Q3: 44.4251, IQR: 9.7762\n",
      "  Bounds: [19.9847, 59.0893]\n",
      "  Outliers found: 38\n",
      "  Outlier range: -200.0000 to 999.0000\n",
      "- longitude:\n",
      "  Q1: -105.2140, Q3: -91.3193, IQR: 13.8946\n",
      "  Bounds: [-126.0559, -70.4774]\n",
      "  Outliers found: 30\n",
      "  Outlier range: -400.0000 to 999.0000\n",
      "\n",
      "Outlier Examples:\n",
      "\n",
      "Latitude outliers (showing first 10):\n",
      "       latitude\n",
      "18  -200.000000\n",
      "42   999.000000\n",
      "104   18.303842\n",
      "126   18.375467\n",
      "127 -200.000000\n",
      "133 -200.000000\n",
      "196 -200.000000\n",
      "347  200.000000\n",
      "393  200.000000\n",
      "400  200.000000\n",
      "\n",
      "Longitude outliers (showing first 10):\n",
      "      longitude\n",
      "7    999.000000\n",
      "115  999.000000\n",
      "198  400.000000\n",
      "213  -69.861749\n",
      "224 -400.000000\n",
      "349 -400.000000\n",
      "431 -400.000000\n",
      "456 -400.000000\n",
      "485  400.000000\n",
      "535  999.000000\n",
      "\n",
      "Example snippet (toy data) - as requested:\n",
      "Toy example outlier detection:\n",
      "Data: [1, 2, 2, 3, 100]\n",
      "Q1: 2.0, Q3: 3.0, IQR: 1.0\n",
      "Outliers: [100]\n"
     ]
    }
   ],
   "source": [
    "# Task 17: Check for outliers in Latitude/Longitude using IQR\n",
    "print(\"Outlier Detection in Geographic Coordinates\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Function to detect outliers using IQR method\n",
    "def detect_outliers_iqr(series, column_name):\n",
    "    \"\"\"Detect outliers using the IQR method\"\"\"\n",
    "    # Remove NaN values for calculation\n",
    "    clean_series = series.dropna()\n",
    "    \n",
    "    if len(clean_series) == 0:\n",
    "        print(f\"- {column_name}: No valid data for outlier detection\")\n",
    "        return pd.Series([], dtype=bool)\n",
    "    \n",
    "    # Calculate quartiles and IQR\n",
    "    q1 = clean_series.quantile(0.25)\n",
    "    q3 = clean_series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Calculate outlier bounds\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = (series < lower_bound) | (series > upper_bound)\n",
    "    outlier_count = outliers.sum()\n",
    "    \n",
    "    print(f\"- {column_name}:\")\n",
    "    print(f\"  Q1: {q1:.4f}, Q3: {q3:.4f}, IQR: {iqr:.4f}\")\n",
    "    print(f\"  Bounds: [{lower_bound:.4f}, {upper_bound:.4f}]\")\n",
    "    print(f\"  Outliers found: {outlier_count}\")\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        outlier_values = series[outliers].dropna()\n",
    "        print(f\"  Outlier range: {outlier_values.min():.4f} to {outlier_values.max():.4f}\")\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Check outliers in latitude and longitude\n",
    "geographic_columns = ['latitude', 'longitude']\n",
    "outlier_results = {}\n",
    "\n",
    "for col in geographic_columns:\n",
    "    if col in df.columns:\n",
    "        outliers = detect_outliers_iqr(df[col], col)\n",
    "        outlier_results[col] = outliers\n",
    "    else:\n",
    "        print(f\"Column '{col}' not found in dataset\")\n",
    "\n",
    "# Show some outlier examples\n",
    "print(f\"\\nOutlier Examples:\")\n",
    "for col, outliers in outlier_results.items():\n",
    "    if outliers.sum() > 0:\n",
    "        outlier_data = df[outliers][[col]].head(10)\n",
    "        print(f\"\\n{col.title()} outliers (showing first 10):\")\n",
    "        print(outlier_data)\n",
    "\n",
    "# Example snippet (toy data) - as requested\n",
    "print(f\"\\nExample snippet (toy data) - as requested:\")\n",
    "nums = pd.Series([1, 2, 2, 3, 100])\n",
    "q1, q3 = nums.quantile(0.25), nums.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "outliers = nums[(nums < q1 - 1.5 * iqr) | (nums > q3 + 1.5 * iqr)]\n",
    "print(\"Toy example outlier detection:\")\n",
    "print(f\"Data: {nums.tolist()}\")\n",
    "print(f\"Q1: {q1}, Q3: {q3}, IQR: {iqr}\")\n",
    "print(f\"Outliers: {outliers.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ebb0d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP Code Validation\n",
      "====================\n",
      "Current ZIP code statistics:\n",
      "- Data type: float64\n",
      "- Non-null count: 1726\n",
      "- Null count: 159\n",
      "- Min value: 0.0\n",
      "- Max value: 999999.0\n",
      "- ZIP codes in valid range (501-99950): 1484\n",
      "- ZIP codes with 5 digits (10000-99999): 1484\n",
      "\n",
      "Invalid ZIP codes found: 242\n",
      "Examples of invalid ZIP codes: [123.0, 123.0, 123.0, 0.0, 999999.0, 0.0, 0.0, 123.0, 123.0, 123.0]\n",
      "Marked 401 invalid ZIP codes as NaN\n",
      "\n",
      "ZIP codes < 10000 (might need leading zeros): 205\n",
      "Examples: [123.0, 123.0, 123.0, 0.0, 0.0, 0.0, 123.0, 123.0, 123.0, 123.0]\n",
      "\n",
      "Final ZIP code statistics after validation:\n",
      "- Valid ZIP codes: 1484\n",
      "- Invalid/Missing ZIP codes: 401\n"
     ]
    }
   ],
   "source": [
    "# Task 18: Ensure ZIP codes are valid 5-digit numbers\n",
    "print(\"ZIP Code Validation\")\n",
    "print(\"=\"*20)\n",
    "\n",
    "# Check current ZIP code status\n",
    "print(\"Current ZIP code statistics:\")\n",
    "zip_col = 'zip_code'\n",
    "if zip_col in df.columns:\n",
    "    print(f\"- Data type: {df[zip_col].dtype}\")\n",
    "    print(f\"- Non-null count: {df[zip_col].notna().sum()}\")\n",
    "    print(f\"- Null count: {df[zip_col].isna().sum()}\")\n",
    "    \n",
    "    # Get non-null ZIP codes for analysis\n",
    "    valid_zips = df[zip_col].dropna()\n",
    "    \n",
    "    if len(valid_zips) > 0:\n",
    "        print(f\"- Min value: {valid_zips.min()}\")\n",
    "        print(f\"- Max value: {valid_zips.max()}\")\n",
    "        \n",
    "        # Check for valid 5-digit ZIP codes\n",
    "        # Valid ZIP codes should be between 00501 and 99950\n",
    "        valid_range = (valid_zips >= 501) & (valid_zips <= 99950)\n",
    "        valid_length = (valid_zips >= 10000) & (valid_zips <= 99999)  # 5-digit check\n",
    "        \n",
    "        valid_count = valid_range.sum()\n",
    "        valid_length_count = valid_length.sum()\n",
    "        \n",
    "        print(f\"- ZIP codes in valid range (501-99950): {valid_count}\")\n",
    "        print(f\"- ZIP codes with 5 digits (10000-99999): {valid_length_count}\")\n",
    "        \n",
    "        # Identify invalid ZIP codes\n",
    "        invalid_zips = valid_zips[~valid_range]\n",
    "        if len(invalid_zips) > 0:\n",
    "            print(f\"\\nInvalid ZIP codes found: {len(invalid_zips)}\")\n",
    "            print(f\"Examples of invalid ZIP codes: {invalid_zips.head(10).tolist()}\")\n",
    "            \n",
    "            # Mark invalid ZIP codes as NaN\n",
    "            invalid_mask = ~((df[zip_col] >= 501) & (df[zip_col] <= 99950))\n",
    "            df.loc[invalid_mask, zip_col] = pd.NA\n",
    "            print(f\"Marked {invalid_mask.sum()} invalid ZIP codes as NaN\")\n",
    "        \n",
    "        # Check for ZIP codes that might need leading zeros\n",
    "        short_zips = valid_zips[valid_zips < 10000]\n",
    "        if len(short_zips) > 0:\n",
    "            print(f\"\\nZIP codes < 10000 (might need leading zeros): {len(short_zips)}\")\n",
    "            print(f\"Examples: {short_zips.head(10).tolist()}\")\n",
    "    \n",
    "    print(f\"\\nFinal ZIP code statistics after validation:\")\n",
    "    print(f\"- Valid ZIP codes: {((df[zip_col] >= 501) & (df[zip_col] <= 99950)).sum()}\")\n",
    "    print(f\"- Invalid/Missing ZIP codes: {df[zip_col].isna().sum()}\")\n",
    "else:\n",
    "    print(f\"Column '{zip_col}' not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701db71e",
   "metadata": {},
   "source": [
    "## 7. Derived Features & Export\n",
    "\n",
    "Let's create useful derived features and export the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c4b3c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Derived Features\n",
      "==============================\n",
      "Creating response_time_days feature...\n",
      "Response Time Statistics:\n",
      "count    1885.000000\n",
      "mean       26.623342\n",
      "std        18.795448\n",
      "min        -5.000000\n",
      "25%        10.000000\n",
      "50%        26.000000\n",
      "75%        43.000000\n",
      "max        59.000000\n",
      "Name: response_time_days, dtype: float64\n",
      "\n",
      "Warning: 135 records have negative response times\n",
      "This might indicate data quality issues\n",
      "\n",
      "Examples of negative response times:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_received</th>\n",
       "      <th>date_sent_to_company</th>\n",
       "      <th>response_time_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-03-29</td>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>2024-01-28</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2024-01-27</td>\n",
       "      <td>2024-01-25</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2023-12-03</td>\n",
       "      <td>2023-12-02</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_received date_sent_to_company  response_time_days\n",
       "18    2023-03-29           2023-03-24                  -5\n",
       "20    2024-02-01           2024-01-28                  -4\n",
       "22    2023-05-30           2023-05-27                  -3\n",
       "37    2024-01-27           2024-01-25                  -2\n",
       "71    2023-12-03           2023-12-02                  -1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response Time Distribution:\n",
      "- Same day (0 days): 23\n",
      "- 1-7 days: 231\n",
      "- 8-30 days: 700\n",
      "- Over 30 days: 796\n",
      "- Missing/Invalid: 0\n",
      "\n",
      "Example snippet (toy data) - as requested:\n",
      "Toy example response time calculation:\n",
      "    received       sent  response_days\n",
      "0 2020-01-01 2020-01-05              4\n",
      "1 2020-01-02 2020-01-03              1\n"
     ]
    }
   ],
   "source": [
    "# Task 19: Create response_time_days = Date Sent  Date Received (in days)\n",
    "print(\"Creating Derived Features\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Create response time feature\n",
    "print(\"Creating response_time_days feature...\")\n",
    "\n",
    "# Check if both date columns exist and are datetime\n",
    "date_received_col = 'date_received'\n",
    "date_sent_col = 'date_sent_to_company'\n",
    "\n",
    "if date_received_col in df.columns and date_sent_col in df.columns:\n",
    "    # Calculate response time in days\n",
    "    df['response_time_days'] = (df[date_sent_col] - df[date_received_col]).dt.days\n",
    "    \n",
    "    # Show statistics for the new feature\n",
    "    response_stats = df['response_time_days'].describe()\n",
    "    print(f\"Response Time Statistics:\")\n",
    "    print(response_stats)\n",
    "    \n",
    "    # Check for negative response times (data quality issue)\n",
    "    negative_response = (df['response_time_days'] < 0).sum()\n",
    "    if negative_response > 0:\n",
    "        print(f\"\\nWarning: {negative_response} records have negative response times\")\n",
    "        print(\"This might indicate data quality issues\")\n",
    "        \n",
    "        # Show examples of negative response times\n",
    "        negative_examples = df[df['response_time_days'] < 0][\n",
    "            [date_received_col, date_sent_col, 'response_time_days']\n",
    "        ].head()\n",
    "        print(\"\\nExamples of negative response times:\")\n",
    "        display(negative_examples)\n",
    "    \n",
    "    # Show distribution\n",
    "    print(f\"\\nResponse Time Distribution:\")\n",
    "    print(f\"- Same day (0 days): {(df['response_time_days'] == 0).sum()}\")\n",
    "    print(f\"- 1-7 days: {((df['response_time_days'] >= 1) & (df['response_time_days'] <= 7)).sum()}\")\n",
    "    print(f\"- 8-30 days: {((df['response_time_days'] >= 8) & (df['response_time_days'] <= 30)).sum()}\")\n",
    "    print(f\"- Over 30 days: {(df['response_time_days'] > 30).sum()}\")\n",
    "    print(f\"- Missing/Invalid: {df['response_time_days'].isna().sum()}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Required columns not found: {date_received_col}, {date_sent_col}\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "\n",
    "# Example snippet (toy data) - as requested\n",
    "print(f\"\\nExample snippet (toy data) - as requested:\")\n",
    "dates = pd.DataFrame({\n",
    "    \"received\": pd.to_datetime([\"2020-01-01\", \"2020-01-02\"]),\n",
    "    \"sent\": pd.to_datetime([\"2020-01-05\", \"2020-01-03\"])\n",
    "})\n",
    "dates['response_days'] = (dates['sent'] - dates['received']).dt.days\n",
    "print(\"Toy example response time calculation:\")\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "942b0059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting Useful Columns for Export\n",
      "========================================\n",
      "Columns selected for export: 14\n",
      "Selected columns:\n",
      "- complaint_id\n",
      "- product\n",
      "- issue\n",
      "- company\n",
      "- state\n",
      "- city\n",
      "- zip_code\n",
      "- status\n",
      "- date_received\n",
      "- date_sent_to_company\n",
      "- response_time_days\n",
      "- timely_response?\n",
      "- consumer_disputed?\n",
      "- company_response_to_consumer\n",
      "\n",
      "Export dataset summary:\n",
      "- Shape: (1885, 14)\n",
      "- Memory usage: 0.09 MB\n",
      "\n",
      "Sample of export dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_id</th>\n",
       "      <th>product</th>\n",
       "      <th>issue</th>\n",
       "      <th>company</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>status</th>\n",
       "      <th>date_received</th>\n",
       "      <th>date_sent_to_company</th>\n",
       "      <th>response_time_days</th>\n",
       "      <th>timely_response?</th>\n",
       "      <th>consumer_disputed?</th>\n",
       "      <th>company_response_to_consumer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Metro Loans</td>\n",
       "      <td>Ny</td>\n",
       "      <td>New York</td>\n",
       "      <td>75285.0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>2023-11-10</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>52</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Collection Harassment</td>\n",
       "      <td>Nan</td>\n",
       "      <td>Fl</td>\n",
       "      <td>Miami</td>\n",
       "      <td>43383.0</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "      <td>Nan</td>\n",
       "      <td>Ny</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43864.0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>2023-10-20</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>38</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>Student  Loan</td>\n",
       "      <td>Fees</td>\n",
       "      <td>Metro Loans</td>\n",
       "      <td>Ga</td>\n",
       "      <td>New York</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-17</td>\n",
       "      <td>2023-03-13</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Collection Harassment</td>\n",
       "      <td>United Credit</td>\n",
       "      <td>Pa</td>\n",
       "      <td>New York</td>\n",
       "      <td>62086.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>2023-11-23</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   complaint_id        product                  issue        company state  \\\n",
       "0        100000       Mortgage                    NaN    Metro Loans    Ny   \n",
       "1        100001       Mortgage  Collection Harassment            Nan    Fl   \n",
       "2        100002            NaN                  Other            Nan    Ny   \n",
       "3        100003  Student  Loan                   Fees    Metro Loans    Ga   \n",
       "4        100004    Credit Card  Collection Harassment  United Credit    Pa   \n",
       "\n",
       "       city  zip_code    status date_received date_sent_to_company  \\\n",
       "0  New York   75285.0    Closed    2023-11-10           2024-01-01   \n",
       "1     Miami   43383.0  Resolved    2023-03-12           2023-03-21   \n",
       "2       NaN   43864.0    Closed    2023-10-20           2023-11-27   \n",
       "3  New York       NaN       NaN    2023-02-17           2023-03-13   \n",
       "4  New York   62086.0       NaN    2023-11-21           2023-11-23   \n",
       "\n",
       "   response_time_days timely_response? consumer_disputed?  \\\n",
       "0                  52                N                NaN   \n",
       "1                   9              NaN                  N   \n",
       "2                  38            FALSE                NaN   \n",
       "3                  24              NaN                 No   \n",
       "4                   2              Yes                NaN   \n",
       "\n",
       "  company_response_to_consumer  \n",
       "0                          NaN  \n",
       "1                          NaN  \n",
       "2                          NaN  \n",
       "3                          NaN  \n",
       "4                          NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task 20: Select a subset of useful columns\n",
    "print(\"Selecting Useful Columns for Export\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Define the subset of useful columns\n",
    "useful_columns = [\n",
    "    'complaint_id',\n",
    "    'product', \n",
    "    'issue',\n",
    "    'company',\n",
    "    'state',\n",
    "    'city',\n",
    "    'zip_code',\n",
    "    'status',\n",
    "    'date_received',\n",
    "    'date_sent_to_company',\n",
    "    'response_time_days',\n",
    "    'timely_response?',\n",
    "    'consumer_disputed?',\n",
    "    'company_response_to_consumer'\n",
    "]\n",
    "\n",
    "# Check which columns exist in the dataset\n",
    "existing_columns = []\n",
    "missing_columns = []\n",
    "\n",
    "for col in useful_columns:\n",
    "    if col in df.columns:\n",
    "        existing_columns.append(col)\n",
    "    else:\n",
    "        missing_columns.append(col)\n",
    "\n",
    "print(f\"Columns selected for export: {len(existing_columns)}\")\n",
    "print(\"Selected columns:\")\n",
    "for col in existing_columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"\\nColumns not found in dataset: {missing_columns}\")\n",
    "\n",
    "# Create the subset dataframe\n",
    "df_export = df[existing_columns].copy()\n",
    "\n",
    "print(f\"\\nExport dataset summary:\")\n",
    "print(f\"- Shape: {df_export.shape}\")\n",
    "print(f\"- Memory usage: {df_export.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Show sample of the export dataset\n",
    "print(f\"\\nSample of export dataset:\")\n",
    "display(df_export.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1943f9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting Cleaned Dataset\n",
      "==============================\n",
      " Successfully exported cleaned dataset to: consumer_complaints_cleaned.csv\n",
      "- Rows exported: 1,885\n",
      "- Columns exported: 14\n",
      "- File size verification: 14 columns read back\n",
      "\n",
      "Export Summary:\n",
      "- Original dataset: 1,885 rows, 17 columns\n",
      "- Cleaned dataset: 1,885 rows, 14 columns\n",
      "- Data reduction: 0.0% rows removed\n",
      "- Column reduction: 17.6% columns removed\n",
      "\n",
      "==================================================\n",
      "DATA CLEANING PIPELINE COMPLETED!\n",
      "==================================================\n",
      "Final dataset ready for analysis: consumer_complaints_cleaned.csv\n",
      "Key cleaning operations performed:\n",
      "-  Missing value analysis and handling\n",
      "-  Duplicate removal\n",
      "-  Data type conversions\n",
      "-  Column standardization (snake_case)\n",
      "-  Category standardization\n",
      "-  Null-like token replacement\n",
      "-  Outlier detection\n",
      "-  Data validation (ZIP codes)\n",
      "-  Derived feature creation\n",
      "-  Export to CSV\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 21: Export to CSV\n",
    "print(\"Exporting Cleaned Dataset\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Define output filename\n",
    "output_filename = \"consumer_complaints_cleaned.csv\"\n",
    "\n",
    "try:\n",
    "    # Export the cleaned dataset\n",
    "    df_export.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\" Successfully exported cleaned dataset to: {output_filename}\")\n",
    "    print(f\"- Rows exported: {len(df_export):,}\")\n",
    "    print(f\"- Columns exported: {len(df_export.columns)}\")\n",
    "    \n",
    "    # Verify the export by reading back a few rows\n",
    "    verification = pd.read_csv(output_filename, nrows=3)\n",
    "    print(f\"- File size verification: {len(verification.columns)} columns read back\")\n",
    "    \n",
    "    print(f\"\\nExport Summary:\")\n",
    "    print(f\"- Original dataset: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "    print(f\"- Cleaned dataset: {df_export.shape[0]:,} rows, {df_export.shape[1]} columns\")\n",
    "    print(f\"- Data reduction: {((df.shape[0] - df_export.shape[0]) / df.shape[0] * 100):.1f}% rows removed\")\n",
    "    print(f\"- Column reduction: {((df.shape[1] - df_export.shape[1]) / df.shape[1] * 100):.1f}% columns removed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error exporting dataset: {str(e)}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"DATA CLEANING PIPELINE COMPLETED!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Final dataset ready for analysis: {output_filename}\")\n",
    "print(f\"Key cleaning operations performed:\")\n",
    "print(f\"-  Missing value analysis and handling\")\n",
    "print(f\"-  Duplicate removal\")\n",
    "print(f\"-  Data type conversions\")\n",
    "print(f\"-  Column standardization (snake_case)\")\n",
    "print(f\"-  Category standardization\")\n",
    "print(f\"-  Null-like token replacement\")\n",
    "print(f\"-  Outlier detection\")\n",
    "print(f\"-  Data validation (ZIP codes)\")\n",
    "print(f\"-  Derived feature creation\")\n",
    "print(f\"-  Export to CSV\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
